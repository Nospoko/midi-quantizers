from tokenizers import Regex, Tokenizer, models, trainers, pre_tokenizers

# This is a prototype script for training huggingface tokenizers using MIDI files
# The checkpoints are not compatible with the custom TrainableMidiTokenizer that you can train on
# bpe_review dashboard.


def main():
    tokenizer = Tokenizer(models.BPE())

    # Not using a pre-tokenizer turns out to be very inefficient
    # Split the tokens into groups before training (concatenate only time tokens)
    velocity_splitter = pre_tokenizers.Split("VELOCITY", behavior="merged_with_next")
    note_on_splitter = pre_tokenizers.Split(Regex("NOTE_ON_.."), behavior="isolated")
    note_off_splitter = pre_tokenizers.Split(Regex("NOTE_OFF_.."), behavior="isolated")

    # in the txt file, new records begin with a newline
    end_line_splitter = pre_tokenizers.Split("\n", behavior="removed")

    # We have to use this - we cannot load saved tokenizer otherwise
    byte_level_pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True, use_regex=False)
    tokenizer.pre_tokenizer = pre_tokenizers.Sequence(
        [byte_level_pre_tokenizer, end_line_splitter, velocity_splitter, note_on_splitter, note_off_splitter]
    )

    trainer = trainers.BpeTrainer(max_token_length=512, special_tokens=["<CLS>"])

    print("training...")
    # Huggingface tokenizers can be trained directly on text data from txt files
    # This is a path to a text file generated by dump_maestro_to_txt.py script
    tokenizer.train(["data/maestro-tokenized-one-time-eps-0.015.txt"], trainer=trainer)
    print("saving...")
    tokenizer.save("dumps/trained_one_time_bpe_tokenizer-eps-0.015.json")


if __name__ == "__main__":
    main()
