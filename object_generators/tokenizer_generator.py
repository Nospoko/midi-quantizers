from glob import glob

import streamlit as st

from midi_tokenizers.midi_tokenizer import MidiTokenizer
from midi_trainable_tokenizers.bpe_tokenizer import BpeTokenizer
from object_generators.base_tokenizer_generator import TokenizerFactory, BaseTokenizerGenerator


class BpeTokenizerFactory(TokenizerFactory):
    tokenizer_desc = """
    This tokenizer can be trained on tokens generated by on of the no-loss models,
    which is passed to it by base_tokenizer parameter.

    WARNING: be sure to choose base-tokenizer that was used during training if using pre-trained BPE tokenizer.
    (I will think of a better serialization technique later)

    You can train new tokenizers by messing with scripts/train_bpe.py
    """

    @staticmethod
    def select_parameters() -> dict:
        trained_tokenizers_options = glob("dumps/*.json")
        path = st.selectbox(label="pre-trained tokenizers", options=trained_tokenizers_options)
        return {"path": path}

    @staticmethod
    def create_tokenizer(parameters: dict) -> BpeTokenizer:
        return BpeTokenizer.from_file(**parameters)


class TokenizerGenerator(BaseTokenizerGenerator):
    # append new factories to this dict when new Tokenizers are defined.
    name_to_factory_map = BaseTokenizerGenerator.name_to_factory_map | {"BpeTokenizer": BpeTokenizerFactory()}

    def tokenizer_info(self, name: str):
        return self.name_to_factory_map[name].tokenizer_desc

    def generate_tokenizer_with_streamlit(self, name: str) -> MidiTokenizer:
        factory = self.name_to_factory_map[name]
        parameters = factory.select_parameters()

        return factory.create_tokenizer(parameters)

    def generate_tokenizer(self, name: str, parameters: dict) -> MidiTokenizer:
        factory = self.name_to_factory_map[name]
        return factory.create_tokenizer(parameters)
